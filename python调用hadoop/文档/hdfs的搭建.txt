1. hdfs dfsadmin -report 返回hadoop集群的信息

2.netstat -ntlp  返回可访问的端口

3.hadoop的网页访问  masterip + ：端口

4.单独管理节点
打开：hadoop-daemon.sh start  namenode/datanode  
关闭：hadoop-daemon.sh stop  namenode/datanode  

5.hadoop/etc/hadoop 下的slaves文档管理着datanode
vim slaves
>>填入所有 datanode名字（S1）

6.利用master管理所有子节点
start-dfs.sh 启动所有节点
stop-dfs.sh  关闭所有节点


7.免密登陆
  cd /.ssh
  ssh-keygen -t rsa         一路回车 生成两个文件 一个root的私钥 ，一个公钥，
  ssh-copy-id S1（子节点）   将公钥给子节点，以后就不用给密码了


8. hadoop fs  -ls（Linux的命令）  。。。。
其实是类似linux的

例如  hadoop fs -put     ...    /                    将。。。上传至hdfs系统

9.修改文件备份数量 

vim hdfs-site.xml
添加
>>   <property>
             <name>dfs.replication</name>
             <value>3</value>
        </property>

10.